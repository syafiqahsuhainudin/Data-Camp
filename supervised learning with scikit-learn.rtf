{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1033{\fonttbl{\f0\fnil\fcharset0 Calibri;}}
{\*\generator Riched20 10.0.18362}\viewkind4\uc1 
\pard\sa200\sl276\slmult1\f0\fs22\lang9 from sklearn.neighbours import KNeighboursClassifier\par
\par
knn=KNeighboursClassifier(n_neighbours=6)\par
knn.fit(iris['data'], iris['target'])\par
iris['data'].shape\par
\par
\par
# Import KNeighborsClassifier from sklearn.neighbors\par
from sklearn.neighbors import KNeighborsClassifier\par
\par
# Create arrays for the features and the response variable\par
y = df['party'].values\par
X = df.drop('party', axis=1).values\par
\par
# Create a k-NN classifier with 6 neighbors\par
knn=KNeighborsClassifier(n_neighbors=6)\par
\par
\par
# Fit the classifier to the data\par
knn.fit(X,y)\par
\par
# Import KNeighborsClassifier from sklearn.neighbors\par
from sklearn.neighbors import KNeighborsClassifier \par
\par
# Create arrays for the features and the response variable\par
y = df['party'].values\par
X = df.drop('party',axis=1).values\par
\par
# Create a k-NN classifier with 6 neighbors: knn\par
knn=KNeighborsClassifier(n_neighbors=6)\par
\par
\par
# Fit the classifier to the data\par
X\par
\par
# Predict the labels for the training data X\par
y_pred = knn.predict(X)\par
\par
# Predict and print the label for the new data point X_new\par
new_prediction =  knn.predict(X_new)\par
print("Prediction: \{\}".format(new_prediction))\par
-----\par
from sklearn.model_election import train_test_split\par
X_train,X_text,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=21,stratify=y)\par
\par
----\par
# Import necessary modules\par
from sklearn import datasets\par
import matplotlib.pyplot as plt\par
\par
# Load the digits dataset: digits\par
digits = datasets.load_digits()\par
\par
# Print the keys and DESCR of the dataset\par
print(digits.keys())\par
print(digits.DESCR)\par
\par
# Print the shape of the images and data keys\par
print(digits.images.shape)\par
print(digits.data.shape)\par
\par
# Display digit 1010\par
plt.imshow(digits.images[1010], cmap=plt.cm.gray_r, interpolation='nearest')\par
plt.show()\par
\par
# Import necessary modules\par
from sklearn.neighbors import KNeighborsClassifier\par
from sklearn.model_selection import train_test_split\par
\par
# Create feature and target arrays\par
X = digits.data\par
y = digits.target\par
\par
# Split into training and test set\par
X_train, X_test, y_train, y_test =train_test_split(X,y, test_size = 0.2, random_state=42, stratify=y)\par
\par
# Create a k-NN classifier with 7 neighbors: knn\par
knn=KNeighborsClassifier(n_neighbors=7)\par
\par
# Fit the classifier to the training data\par
knn.fit(X_train, y_train)\par
\par
# Print the accuracy\par
print(knn.score(X_test, y_test))\par
\par
-----\par
# Setup arrays to store train and test accuracies\par
neighbors = np.arange(1, 9)\par
train_accuracy = np.empty(len(neighbors))\par
test_accuracy = np.empty(len(neighbors))\par
\par
# Loop over different values of k\par
for i, k in enumerate(neighbors):\par
 # Setup a k-NN Classifier with k neighbors: knn\par
    knn = KNeighborsClassifier(n_neighbors=k)\par
\par
    # Fit the classifier to the training data\par
    knn.fit(X_train, y_train)\par
    \par
    #Compute accuracy on the training set\par
    train_accuracy[i] = knn.score(X_train, y_train)\par
\par
    #Compute accuracy on the testing set\par
    test_accuracy[i] = knn.score(X_test, y_test)\par
\par
# Generate plot\par
plt.title('k-NN: Varying Number of Neighbors')\par
plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\par
plt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\par
plt.legend()\par
plt.xlabel('Number of Neighbors')\par
plt.ylabel('Accuracy')\par
plt.show()\par
-----\par
# Import numpy and pandas\par
import numpy as np\par
import pandas as pd\par
\par
# Read the CSV file into a DataFrame: df\par
df =pd.read_csv('gapminder.csv')\par
\par
# Create arrays for features and target variable\par
y = df['life'].values\par
X = df['fertility'].values\par
\par
# Print the dimensions of X and y before reshaping\par
print("Dimensions of y before reshaping: \{\}".format(y.shape))\par
print("Dimensions of X before reshaping: \{\}".format(X.shape))\par
\par
# Reshape X and y\par
y =y.reshape(-1,1)\par
X = X.reshape(-1,1)\par
\par
# Print the dimensions of X and y after reshaping\par
print("Dimensions of y after reshaping: \{\}".format(y.shape))\par
print("Dimensions of X after reshaping: \{\}".format(X.shape))\par
-----\par
# Import LinearRegression\par
from sklearn.linear_model import LinearRegression\par
\par
# Create the regressor: reg\par
reg = LinearRegression()\par
\par
\par
# Create the prediction space\par
prediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1,1)\par
\par
# Fit the model to the data\par
reg.fit(X_fertility, y)\par
\par
# Compute predictions over the prediction space: y_pred\par
y_pred = reg.predict(prediction_space)\par
\par
# Print R^2 \par
print(reg.score(X_fertility, y))\par
\par
# Plot regression line\par
plt.plot(prediction_space, y_pred, color='black', linewidth=3)\par
plt.show()\par
\par
\par
# Import necessary modules\par
from sklearn.linear_model import LinearRegression\par
from sklearn.metrics import mean_squared_error\par
from sklearn.model_selection import train_test_split\par
\par
# Create training and test sets\par
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state=42)\par
\par
# Create the regressor: reg_all\par
reg_all =  LinearRegression()\par
\par
# Fit the regressor to the training data\par
reg_all.fit(X_train, y_train)\par
\par
# Predict on the test data: y_pred\par
y_pred = reg_all.predict(X_test)\par
\par
# Compute and print R^2 and RMSE\par
print("R^2: \{\}".format(reg_all.score(X_test, y_test)))\par
rmse = np.sqrt(mean_squared_error(y_test, y_pred))\par
print("Root Mean Squared Error: \{\}".format(rmse))\par
----\par
# Import the necessary modules\par
from sklearn.linear_model import LinearRegression\par
from sklearn.model_selection import cross_val_score\par
\par
\par
# Create a linear regression object: reg\par
reg = LinearRegression()\par
\par
# Compute 5-fold cross-validation scores: cv_scores\par
cv_scores = cross_val_score(reg,X,y,cv=5)\par
\par
# Print the 5-fold cross-validation scores\par
print(cv_scores)\par
\par
print("Average 5-Fold CV Score: \{\}".format(np.mean(cv_scores)))\par
-----\par
# Import necessary modules\par
from sklearn.linear_model import LinearRegression\par
from sklearn.model_selection import cross_val_score\par
\par
# Create a linear regression object: reg\par
reg = LinearRegression()\par
\par
# Perform 3-fold CV\par
cvscores_3 = cross_val_score(reg, X, y, cv=3)\par
print(np.mean(cvscores_3))\par
\par
# Perform 10-fold CV\par
cvscores_10 = cross_val_score(reg, X, y, cv=10)\par
print(np.mean(cvscores_10))\par
----\par
# Import Lasso\par
from sklearn.linear_model import Lasso\par
# Instantiate a lasso regressor: lasso\par
lasso = Lasso(alpha=0.4, normalize=True)\par
\par
# Fit the regressor to the data\par
lasso.fit(X,y,)\par
\par
# Compute and print the coefficients\par
lasso_coef = lasso.coef_\par
print(lasso_coef)\par
\par
# Plot the coefficients\par
plt.plot(range(len(df_columns)), lasso_coef)\par
plt.xticks(range(len(df_columns)), df_columns.values, rotation=60)\par
plt.margins(0.02)\par
plt.show()\par
-----\par
# Import necessary modules\par
from sklearn.linear_model import Ridge\par
from sklearn.model_selection import cross_val_score\par
\par
# Setup the array of alphas and lists to store scores\par
alpha_space = np.logspace(-4, 0, 50)\par
ridge_scores = []\par
ridge_scores_std = []\par
\par
# Create a ridge regressor: ridge\par
ridge = Ridge(normalize=True)\par
\par
# Compute scores over range of alphas\par
for alpha in alpha_space:\par
\par
    # Specify the alpha value to use: ridge.alpha\par
    ridge.alpha = alpha\par
    \par
    # Perform 10-fold CV: ridge_cv_scores\par
    ridge_cv_scores = cross_val_score(ridge, X, y, cv=10)\par
    \par
    # Append the mean of ridge_cv_scores to ridge_scores\par
    ridge_scores.append(np.mean(ridge_cv_scores))\par
    \par
    # Append the std of ridge_cv_scores to ridge_scores_std\par
    ridge_scores_std.append(np.std(ridge_cv_scores))\par
\par
# Display the plot\par
display_plot(ridge_scores, ridge_scores_std)\par
-----\par
# Import necessary modules\par
from sklearn.metrics import classification_report\par
from sklearn.metrics import confusion_matrix\par
\par
# Create training and test set\par
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4,random_state=42)\par
\par
# Instantiate a k-NN classifier: knn\par
knn = KNeighborsClassifier(n_neighbors=6)\par
\par
# Fit the classifier to the training data\par
knn.fit(X_train, y_train)\par
\par
# Predict the labels of the test data: y_pred\par
y_pred = knn.predict(X_test)\par
\par
# Generate the confusion matrix and classification report\par
print(confusion_matrix(y_test, y_pred))\par
print(classification_report(y_test, y_pred))\par
-----\par
# Import the necessary modules\par
from sklearn.linear_model import LogisticRegression\par
from sklearn.metrics import confusion_matrix, classification_report\par
\par
# Create training and test sets\par
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)\par
\par
# Create the classifier: logreg\par
logreg = LogisticRegression()\par
\par
# Fit the classifier to the training data\par
logreg.fit(X_train, y_train)\par
\par
# Predict the labels of the test set: y_pred\par
y_pred = logreg.predict(X_test)\par
\par
# Compute and print the confusion matrix and classification report\par
print(confusion_matrix(y_test, y_pred))\par
print(classification_report(y_test, y_pred))\par
\par
----\par
# Import necessary modules\par
from sklearn.metrics import roc_curve\par
\par
# Compute predicted probabilities: y_pred_prob\par
y_pred_prob = logreg.predict_proba(X_test)[:,1]\par
\par
# Generate ROC curve values: fpr, tpr, thresholds\par
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\par
\par
# Plot ROC curve\par
plt.plot([0, 1], [0, 1], 'k--')\par
plt.plot(fpr, tpr)\par
plt.xlabel('False Positive Rate')\par
plt.ylabel('True Positive Rate')\par
plt.title('ROC Curve')\par
plt.show()\par
----\par
# Import necessary modules\par
from sklearn.metrics import roc_auc_score\par
from sklearn.model_selection import cross_val_score\par
\par
# Compute predicted probabilities: y_pred_prob\par
y_pred_prob = logreg.predict_proba(X_test)[:,1]\par
\par
# Compute and print AUC score\par
print("AUC: \{\}".format(roc_auc_score(y_test, y_pred_prob)))\par
\par
# Compute cross-validated AUC scores: cv_auc\par
cv_auc = cross_val_score(logreg, X, y, cv=5, scoring='roc_auc')\par
\par
# Print list of AUC scores\par
print("AUC scores computed using 5-fold cross-validation: \{\}".format(cv_auc))\par
-----\par
# Import necessary modules\par
from sklearn.linear_model import LogisticRegression\par
from sklearn.model_selection import GridSearchCV\par
\par
# Setup the hyperparameter grid\par
c_space = np.logspace(-5, 8, 15)\par
param_grid = \{'C': c_space\}\par
\par
# Instantiate a logistic regression classifier: logreg\par
logreg = LogisticRegression()\par
\par
# Instantiate the GridSearchCV object: logreg_cv\par
logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\par
\par
# Fit it to the data\par
logreg_cv.fit(X, y)\par
\par
# Print the tuned parameters and score\par
print("Tuned Logistic Regression Parameters: \{\}".format(logreg_cv.best_params_)) \par
print("Best score is \{\}".format(logreg_cv.best_score_))\par
\par
----\par
# Import necessary modules\par
from scipy.stats import randint\par
from sklearn.tree import DecisionTreeClassifier\par
from sklearn.model_selection import RandomizedSearchCV\par
\par
# Setup the parameters and distributions to sample from: param_dist\par
param_dist = \{"max_depth": [3, None],\par
              "max_features": randint(1, 9),\par
              "min_samples_leaf": randint(1, 9),\par
              "criterion": ["gini", "entropy"]\}\par
\par
# Instantiate a Decision Tree classifier: tree\par
tree = DecisionTreeClassifier()\par
\par
# Instantiate the RandomizedSearchCV object: tree_cv\par
tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\par
\par
# Fit it to the data\par
tree_cv.fit(X, y)\par
\par
# Print the tuned parameters and score\par
print("Tuned Decision Tree Parameters: \{\}".format(tree_cv.best_params_))\par
print("Best score is \{\}".format(tree_cv.best_score_))\par
----\par
# Import necessary modules\par
from sklearn.model_selection import train_test_split\par
from sklearn.linear_model import LogisticRegression\par
from sklearn.model_selection import GridSearchCV\par
\par
# Create the hyperparameter grid\par
c_space = np.logspace(-5, 8, 15)\par
param_grid = \{'C': c_space, 'penalty': ['l1', 'l2']\}\par
\par
# Instantiate the logistic regression classifier: logreg\par
logreg = LogisticRegression()\par
\par
# Create train and test sets\par
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)\par
\par
# Instantiate the GridSearchCV object: logreg_cv\par
logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\par
\par
# Fit it to the training data\par
logreg_cv.fit(X_train, y_train)\par
\par
# Print the optimal parameters and best score\par
print("Tuned Logistic Regression Parameter: \{\}".format(logreg_cv.best_params_))\par
print("Tuned Logistic Regression Accuracy: \{\}".format(logreg_cv.best_score_))\par
\par
----\par
# Import necessary modules\par
from sklearn.linear_model import ElasticNet\par
from sklearn.metrics import mean_squared_error\par
from sklearn.model_selection import GridSearchCV\par
from sklearn.model_selection import train_test_split\par
\par
# Create train and test sets\par
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.40, random_state = 42)\par
\par
# Create the hyperparameter grid\par
l1_space = np.linspace(0, 1, 30)\par
param_grid = \{'l1_ratio': l1_space\}\par
\par
# Instantiate the ElasticNet regressor: elastic_net\par
elastic_net = ElasticNet()\par
\par
# Setup the GridSearchCV object: gm_cv\par
gm_cv = GridSearchCV(elastic_net, param_grid, cv=5)\par
\par
# Fit it to the training data\par
gm_cv.fit(X_train, y_train)\par
\par
# Predict on the test set and compute metrics\par
y_pred = gm_cv.predict(X_test)\par
r2 = gm_cv.score(X_test, y_test)\par
mse = mean_squared_error(y_test, y_pred)\par
print("Tuned ElasticNet l1 ratio: \{\}".format(gm_cv.best_params_))\par
print("Tuned ElasticNet R squared: \{\}".format(r2))\par
print("Tuned ElasticNet MSE: \{\}".format(mse))\par
\par
----\par
# Create dummy variables: df_region\par
df_region = pd.get_dummies(df)\par
\par
# Print the columns of df_region\par
print(df_region.columns)\par
\par
# Create dummy variables with drop_first=True: df_region\par
df_region = pd.get_dummies(df, drop_first=True)\par
\par
# Print the new columns of df_region\par
print(df_region.columns)\par
\par
# Import necessary modules\par
from sklearn.linear_model import Ridge\par
from sklearn.model_selection import cross_val_score\par
\par
# Instantiate a ridge regressor: ridge\par
ridge = Ridge(normalize=True, alpha=0.5)\par
\par
# Perform 5-fold cross-validation: ridge_cv\par
ridge_cv = cross_val_score(ridge, X, y, cv=5)\par
\par
# Print the cross-validated scores\par
print(ridge_cv)\par
\par
# Convert '?' to NaN\par
df[df == '?'] = np.nan\par
\par
# Print the number of NaNs\par
print(df.isnull().sum())\par
\par
# Print shape of original DataFrame\par
print("Shape of Original DataFrame: \{\}".format(df.shape))\par
\par
# Drop missing values and print shape of new DataFrame\par
df = df.dropna()\par
\par
# Print shape of new DataFrame\par
print("Shape of DataFrame After Dropping All Rows with Missing Values: \{\}".format(df.shape))\par
\par
\par
\par
----\par
# Import the Imputer module\par
from sklearn.preprocessing import Imputer\par
from sklearn.svm import SVC\par
\par
# Setup the Imputation transformer: imp\par
imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\par
\par
# Instantiate the SVC classifier: clf\par
clf = SVC()\par
\par
# Setup the pipeline with the required steps: steps\par
steps = [('imputation', imp),\par
        ('SVM', clf)]\par
\par
\par
\par
------\par
# Import necessary modules\par
from sklearn.preprocessing import Imputer\par
from sklearn.pipeline import Pipeline\par
from sklearn.svm import SVC\par
\par
# Setup the pipeline steps: steps\par
steps = [('imputation', Imputer(missing_values='NaN', strategy='most_frequent', axis=0)),\par
        ('SVM', SVC())]\par
\par
# Create the pipeline: pipeline\par
pipeline = Pipeline(steps)\par
\par
# Create training and test sets\par
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\par
\par
# Fit the pipeline to the train set\par
pipeline.fit(X_train, y_train)\par
\par
# Predict the labels of the test set\par
y_pred = pipeline.predict(X_test)\par
\par
# Compute metrics\par
print(classification_report(y_test, y_pred))\par
----\par
# Import the necessary modules\par
from sklearn.preprocessing import StandardScaler\par
from sklearn.pipeline import Pipeline\par
\par
# Setup the pipeline steps: steps\par
steps = [('scaler', StandardScaler()),\par
        ('knn', KNeighborsClassifier())]\par
        \par
# Create the pipeline: pipeline\par
pipeline = Pipeline(steps)\par
\par
# Create train and test sets\par
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\par
\par
# Fit the pipeline to the training set: knn_scaled\par
knn_scaled = pipeline.fit(X_train, y_train)\par
\par
# Instantiate and fit a k-NN classifier to the unscaled data\par
knn_unscaled = KNeighborsClassifier().fit(X_train, y_train)\par
\par
# Compute and print metrics\par
print('Accuracy with Scaling: \{\}'.format(knn_scaled.score(X_test, y_test)))\par
print('Accuracy without Scaling: \{\}'.format(knn_unscaled.score(X_test, y_test)))\par
\par
-----\par
#Setup the pipeline\par
steps = [('scaler', StandardScaler()),\par
         ('SVM', SVC())]\par
\par
pipeline = Pipeline(steps)\par
\par
# Specify the hyperparameter space\par
parameters = \{'SVM__C':[1, 10, 100],\par
              'SVM__gamma':[0.1, 0.01]\}\par
\par
# Create train and test sets\par
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=21)\par
\par
# Instantiate the GridSearchCV object: cv\par
cv = GridSearchCV(pipeline, param_grid=parameters)\par
\par
# Fit to the training set\par
cv.fit(X_train, y_train)\par
\par
# Predict the labels of the test set: y_pred\par
y_pred = cv.predict(X_test)\par
\par
# Compute and print metrics\par
print("Accuracy: \{\}".format(cv.score(X_test, y_test)))\par
print(classification_report(y_test, y_pred))\par
print("Tuned Model Parameters: \{\}".format(cv.best_params_))\par
}
 